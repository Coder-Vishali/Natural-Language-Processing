{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDCXW4o3ITeL"
   },
   "source": [
    "# NLP - Machine Translation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyxQDBxYMu5E"
   },
   "source": [
    "# Intoduction & Loading of packages and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHAVXxIBI8wg"
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "Prepare a python notebook (recommended- use Google Colab) to build, train and evaluate a deep neural network that functions as a part of an end-to-end machine translation pipeline that will accept English text as input and return the Korean translation. Read the instructions carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlFaXSa8KJUM"
   },
   "source": [
    "### Checking the availability of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJI7XA8W6uvR",
    "outputId": "23dd3725-e598-437e-c19f-538c87995b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 13 08:56:56 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqd6dctjJIIu"
   },
   "source": [
    "### Installing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3FEE-Y680Tf3",
    "outputId": "86fa9b96-7268-4d1a-8946-a63a990d90eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.1)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.63.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (0.8.9)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install kaggle\n",
    "! pip install pandas\n",
    "! pip install tabulate\n",
    "! pip install keras\n",
    "! pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gotwdhs_JM3q"
   },
   "source": [
    "#### Uploading & Unzipping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vy8myP3sM5Jl",
    "outputId": "89ed7ac4-a59d-4981-9d47-578665a78a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  archive.zip\n",
      "  inflating: multitarget-ted/en-ko/raw/ted_dev_en-ko.raw.en  \n",
      "  inflating: multitarget-ted/en-ko/raw/ted_dev_en-ko.raw.ko  \n",
      "  inflating: multitarget-ted/en-ko/raw/ted_test1_en-ko.raw.en  \n",
      "  inflating: multitarget-ted/en-ko/raw/ted_test1_en-ko.raw.ko  \n",
      "  inflating: multitarget-ted/en-ko/raw/ted_train_en-ko.raw.en  \n",
      "  inflating: multitarget-ted/en-ko/raw/ted_train_en-ko.raw.ko  \n"
     ]
    }
   ],
   "source": [
    "!unzip archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIZEFDsMJUjl"
   },
   "source": [
    "### Downloading stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EktfMgWasTkh",
    "outputId": "05a88243-8940-429a-8482-35a216dfbfdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gF-nJnB1Jtut"
   },
   "source": [
    "### Importing the required packages\n",
    "\n",
    "We have used keras & tensorflow to implement the version of RNN to prepare our own tokenizers, word embeddings & model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gbura2m-ThLP"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tabulate import tabulate\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W830amUjKFvC"
   },
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eX9uDQHmYCDv",
    "outputId": "bb45201e-fc64-45b6-8412-2eba40bea2ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "with open(\"multitarget-ted/en-ko/raw/ted_train_en-ko.raw.en\", \"r\") as english_train_file:\n",
    "    english_sentences_train = english_train_file.readlines()\n",
    "\n",
    "with open(\"multitarget-ted/en-ko/raw/ted_train_en-ko.raw.ko\", \"r\") as korean_train_file:\n",
    "    korean_sentences_train = korean_train_file.readlines()\n",
    "\n",
    "with open(\"multitarget-ted/en-ko/raw/ted_test1_en-ko.raw.en\", \"r\") as english_test_file:\n",
    "    english_sentences_test = english_test_file.readlines()\n",
    "\n",
    "with open(\"multitarget-ted/en-ko/raw/ted_test1_en-ko.raw.ko\", \"r\") as korean_test_file:\n",
    "    korean_sentences_test = korean_test_file.readlines()\n",
    "\n",
    "train_data = pd.DataFrame(\n",
    "    {\"english_sentences_train\": english_sentences_train,\n",
    "     \"korean_sentences_train\": korean_sentences_train,\n",
    "    })\n",
    "\n",
    "print(\"Dataset Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50y9TehBKhnS"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OngZDzMxMh7a"
   },
   "source": [
    "### Printing 5 rows - to perform sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z3wL2YH4iLZR",
    "outputId": "83d801ae-6a82-4fd6-ae22-fbc5a00a123e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "| 0 | (Applause) David Gallo: This is Bill Lange. I'm Dave Gallo.                                                                                                           | (박수) 이쪽은 Bill Lange 이고, 저는 David Gallo입니다                                                                  |\n",
      "| 1 | And we're going to tell you some stories from the sea here in video.                                                                                                  | 우리는 여러분에게 바닷속 이야기를 영상과 함께 들려주고자 합니다.                                                       |\n",
      "| 2 | We've got some of the most incredible video of Titanic that's ever been seen, and we're not going to show you any of it.                                              | 저희는 끝내주는 타이타닉 비디오도 있긴 합니다만 뭐..여기서는 눈꼽만큼도 보여줄 생각이없습니다.                         |\n",
      "| 3 | (Laughter) The truth of the matter is that the Titanic -- even though it's breaking all sorts of box office records -- it's not the most exciting story from the sea. | (웃음) 비록 타이타닉이 박스오피스에서 굉장한 실적을 거두긴 했지만 바다가 들려주는 이야기 중 가장 재밌는 것은 아닙니다. |\n",
      "| 4 | And the problem, I think, is that we take the ocean for granted.                                                                                                      | 문제라면 우리는 우리가 바다를 이미 알고있다고 믿는거죠.                                                                |\n",
      "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(train_data[:5], tablefmt=\"psql\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY6YJgBMM8bI"
   },
   "source": [
    "### Checking the number of sentences in train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7jjq-eelB84",
    "outputId": "5589f9a2-ea15-4458-d486-3a9f244f27c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in English train file -  166215\n",
      "Number of sentences in Korean train file -  166215\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences in English train file - \", len(english_sentences_train)) \n",
    "print(\"Number of sentences in Korean train file - \", len(korean_sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PO1HAkpjmBC9",
    "outputId": "0bb31c7d-f25b-4276-e420-4ff03c80eb93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in English test file -  1982\n",
      "Number of sentences in Korean test file -  1982\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences in English test file - \", len(english_sentences_test)) \n",
    "print(\"Number of sentences in Korean test file - \", len(korean_sentences_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXHK7kNPNCdb"
   },
   "source": [
    "### Exploration of English train file\n",
    "\n",
    "The following exploration have been performed on the data:\n",
    "\n",
    "\n",
    "1.   Total number of words\n",
    "2.   Total number of unique words\n",
    "3.   10 most common words\n",
    "4.   10 most common words when stopwords are excluded\n",
    "5.   Average number of words per sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EfAmJtERmOI_",
    "outputId": "73335584-fd09-4f2f-967c-28eb6dfeb2a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of English tokens - 2910762\n",
      "Total unique tokens - 105027\n",
      "10 Most common words in the English dataset: \"the\", \"and\", \"to\", \"of\", \"a\", \"that\", \"in\", \"i\", \"is\", \"you\"\n",
      "10 Most common words in the English dataset without stop words: \"(applause)\", \"david\", \"gallo:\", \"bill\", \"lange.\", \"i'm\", \"dave\", \"gallo.\", \"we're\", \"going\"\n",
      "Average number of tokens per sentence - 18\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word.lower() for sentence in english_sentences_train for word in sentence.split()])\n",
    "total_number_of_tokens = len([word.lower() for sentence in english_sentences_train for word in sentence.split()])\n",
    "common_word_list = []\n",
    "count = 0\n",
    "for word in english_words_counter:\n",
    "    if word not in stop_words:\n",
    "        common_word_list.append(word)\n",
    "        count += 1\n",
    "    if count == 10:\n",
    "        break\n",
    "print(\"Total number of English tokens - {}\".format(total_number_of_tokens))\n",
    "print(\"Total unique tokens - {}\".format(len(english_words_counter)))\n",
    "print(\"10 Most common words in the English dataset:\", '\"' + '\", \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print(\"10 Most common words in the English dataset without stop words:\", '\"' + '\", \"'.join(common_word_list) + '\"')\n",
    "print(\"Average number of tokens per sentence - {}\".format(round(total_number_of_tokens / len(english_sentences_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmX4L1pbNhI9"
   },
   "source": [
    "### Exploration of Korean train file\n",
    "\n",
    "The following exploration have been performed on the data:\n",
    "\n",
    "\n",
    "1.   Total number of words\n",
    "2.   Total number of unique words\n",
    "3.   10 most common words\n",
    "5.   Average number of words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GphBxNiUme2f",
    "outputId": "0e0afcb1-0594-496b-931f-28fa29b10caf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Korean tokens - 2026438\n",
      "Total unique tokens - 330791\n",
      "10 Most common words in the Korean dataset: \"수\", \"이\", \"그\", \"그리고\", \"있습니다.\", \"저는\", \"있는\", \"우리는\", \"제가\", \"우리가\"\n",
      "Average number of tokens per sentence - 12\n"
     ]
    }
   ],
   "source": [
    "korean_words_counter = collections.Counter([word for sentence in korean_sentences_train for word in sentence.split()])\n",
    "total_number_of_korean_tokens = len([word for sentence in korean_sentences_train for word in sentence.split()])\n",
    "print(\"Total number of Korean tokens - {}\".format(total_number_of_korean_tokens))\n",
    "print(\"Total unique tokens - {}\".format(len(korean_words_counter)))\n",
    "print(\"10 Most common words in the Korean dataset:\", '\"' + '\", \"'.join(list(zip(*korean_words_counter.most_common(10)))[0]) + '\"')\n",
    "print(\"Average number of tokens per sentence - {}\".format(round(total_number_of_korean_tokens / len(korean_sentences_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3nGnIErNnRB"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMY-iWH7Nre0"
   },
   "source": [
    "### Preprocessing of text\n",
    "\n",
    "During preprocessing of text all the special characters, new lines & tabs have been removed from them. Stopwords have not been removed because stopwords form an important part of a given language (in this case English) & removing them would impact the translation efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "85ID3I-4tszA"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text_list):\n",
    "    cleaned_list = []\n",
    "    for sentence in text_list:\n",
    "        sentence = sentence.replace(\"\\\"\", \"\").replace(\"!\", \"\").replace(\"(\", \"\").replace(\"@\", \"\").replace(\"#\", \"\").replace(\"$\", \"\").replace(\"%\", \"\").replace(\"^\", \"\").replace(\"&\", \"\").replace(\"*\", \"\")\n",
    "        sentence = sentence.replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"{\", \"\").replace(\"}\", \"\").replace(\":\", \"\").replace(\";\", \"\").replace(\",\", \"\").replace(\".\", \"\").replace(\"/\", \"\").replace(\"'\", \"\")\n",
    "        sentence = sentence.replace(\"<\", \"\").replace(\">\", \"\").replace(\"?\", \"\").replace(\"\\\\\", \"\").replace(\"|\", \"\").replace(\"`\", \"\").replace(\"~\", \"\").replace(\"=\", \"\").replace(\"_\", \"\").replace(\"+\", \"\").replace(\"-\", \"\")\n",
    "        sentence = sentence.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "        sentence = sentence.replace(\"\\t\", \"\")\n",
    "        sentence = ' '.join(sentence.split())\n",
    "        cleaned_list.append(sentence)\n",
    "    return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azBVDQGaxhSZ",
    "outputId": "9f69b912-394d-46d1-f345-309b5ac76b4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"(Applause) David Gallo: This is Bill Lange. I'm Dave Gallo. \\n\", \"And we're going to tell you some stories from the sea here in video. \\n\", \"We've got some of the most incredible video of Titanic that's ever been seen, and we're not going to show you any of it. \\n\", \"(Laughter) The truth of the matter is that the Titanic -- even though it's breaking all sorts of box office records -- it's not the most exciting story from the sea. \\n\", 'And the problem, I think, is that we take the ocean for granted. \\n']\n",
      "['Applause David Gallo This is Bill Lange Im Dave Gallo', 'And were going to tell you some stories from the sea here in video', 'Weve got some of the most incredible video of Titanic thats ever been seen and were not going to show you any of it', 'Laughter The truth of the matter is that the Titanic even though its breaking all sorts of box office records its not the most exciting story from the sea', 'And the problem I think is that we take the ocean for granted']\n"
     ]
    }
   ],
   "source": [
    "print(english_sentences_train[:5])\n",
    "english_sentences_train = preprocess_text(english_sentences_train)\n",
    "english_sentences_test = preprocess_text(english_sentences_test)\n",
    "print(english_sentences_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiSrE-AhxsX4",
    "outputId": "9555abc4-098e-419f-d462-402c6f8d791e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(박수) 이쪽은 Bill Lange 이고, 저는 David Gallo입니다\\n', '우리는 여러분에게 바닷속 이야기를 영상과 함께 들려주고자 합니다.\\n', '저희는 끝내주는 타이타닉 비디오도 있긴 합니다만 뭐..여기서는 눈꼽만큼도 보여줄 생각이없습니다.\\n', '(웃음) 비록 타이타닉이 박스오피스에서 굉장한 실적을 거두긴 했지만 바다가 들려주는 이야기 중 가장 재밌는 것은 아닙니다.\\n', '문제라면 우리는 우리가 바다를 이미 알고있다고 믿는거죠.\\n']\n",
      "['박수 이쪽은 Bill Lange 이고 저는 David Gallo입니다', '우리는 여러분에게 바닷속 이야기를 영상과 함께 들려주고자 합니다', '저희는 끝내주는 타이타닉 비디오도 있긴 합니다만 뭐여기서는 눈꼽만큼도 보여줄 생각이없습니다', '웃음 비록 타이타닉이 박스오피스에서 굉장한 실적을 거두긴 했지만 바다가 들려주는 이야기 중 가장 재밌는 것은 아닙니다', '문제라면 우리는 우리가 바다를 이미 알고있다고 믿는거죠']\n"
     ]
    }
   ],
   "source": [
    "print(korean_sentences_train[:5])\n",
    "korean_sentences_train = preprocess_text(korean_sentences_train)\n",
    "korean_sentences_test = preprocess_text(korean_sentences_test)\n",
    "print(korean_sentences_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8AkT3IsOLDI"
   },
   "source": [
    "### Tokenize & Padding of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yzZtACVsyUnA"
   },
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    x_tk = Tokenizer(char_level = False)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VXVIChQBG1Qg"
   },
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = length, padding = 'post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "s7-GNqFPG2In"
   },
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "svh3vJA0HLoY"
   },
   "outputs": [],
   "source": [
    "preproc_english_sentences_train, preproc_korean_sentences_train, english_tokenizer_train, korean_tokenizer_train = preprocess(english_sentences_train, korean_sentences_train)\n",
    "preproc_english_sentences_test, preproc_korean_sentences_test, english_tokenizer_test, korean_tokenizer_test = preprocess(english_sentences_test, korean_sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5165uuCnORLh"
   },
   "source": [
    "### Checking the maximum sentence length & vocabulary size of both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Ty8sCPpHU4b",
    "outputId": "461a41a1-2751-4e41-96ef-bb820927604e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed\n",
      "Max English sentence length: 114\n",
      "Max Korean sentence length: 108\n",
      "English vocabulary size: 53584\n",
      "Korean vocabulary size: 292902\n"
     ]
    }
   ],
   "source": [
    "max_english_sequence_length = preproc_english_sentences_train.shape[1]\n",
    "max_korean_sequence_length = preproc_korean_sentences_train.shape[1]\n",
    "english_vocab_size = len(english_tokenizer_train.word_index)\n",
    "korean_vocab_size = len(korean_tokenizer_train.word_index)\n",
    "\n",
    "print('Data preprocessing completed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max Korean sentence length:\", max_korean_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"Korean vocabulary size:\", korean_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBR1oNnyOdvF"
   },
   "source": [
    "# Building & Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD53M7ydOkH3"
   },
   "source": [
    "The model has been trained using RNN with embeddings formed by the words present in both the datasets. Since the dataset is huge in size & there might be a lot of words repeating, the model stabilizes relatively quickly. After running for 5 epochs, both train & validation accuracy stays near 89%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "1_bfjhNuHq4p"
   },
   "outputs": [],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, korean_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    rnn = GRU(64, return_sequences=True, activation=\"tanh\")\n",
    "    \n",
    "    embedding = Embedding(korean_vocab_size, 64, input_length=input_shape[1]) \n",
    "    logits = TimeDistributed(Dense(korean_vocab_size, activation=\"softmax\"))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(rnn)\n",
    "    model.add(logits)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate), \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences_train, max_english_sequence_length)\n",
    "tmp_y = pad(preproc_korean_sentences_train, max_english_sequence_length)\n",
    "embeded_model = embed_model(tmp_x.shape, max_english_sequence_length, english_vocab_size, korean_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9cKAVYtI7NI",
    "outputId": "5574a066-748a-44c9-a129-f58062208845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6234/6234 [==============================] - 6499s 1s/step - loss: 1.3075 - accuracy: 0.8873 - val_loss: nan - val_accuracy: 0.8920\n",
      "Epoch 2/5\n",
      "6234/6234 [==============================] - 6493s 1s/step - loss: 1.1532 - accuracy: 0.8879 - val_loss: nan - val_accuracy: 0.8922\n",
      "Epoch 3/5\n",
      "6234/6234 [==============================] - 6493s 1s/step - loss: 1.1254 - accuracy: 0.8881 - val_loss: nan - val_accuracy: 0.8922\n",
      "Epoch 4/5\n",
      "6234/6234 [==============================] - 6493s 1s/step - loss: 1.1003 - accuracy: 0.8882 - val_loss: nan - val_accuracy: 0.8923\n",
      "Epoch 5/5\n",
      "6234/6234 [==============================] - 6494s 1s/step - loss: 1.0796 - accuracy: 0.8884 - val_loss: nan - val_accuracy: 0.8922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f24e4c23690>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_model.fit(tmp_x, tmp_y, batch_size=24, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xYvzNbAPCGM"
   },
   "source": [
    "# Prediction\n",
    "\n",
    "One of the ways to evaluate a language translation model would be to have a bidirectional language translation model. For example, a sentence would be translated from English->Korean->English to check the efficiency of the model. Building 2 models to perform language translation would require a lot of intensive resources & hence we have used the help from google translate to perform the translation & check the accuracy of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EaJ1lcyXcZPe",
    "outputId": "6e014f19-254b-49b4-ca19-94c8ffe532ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박수 감사합니다                                                                                                                \n"
     ]
    }
   ],
   "source": [
    "sentence_list = [\"Thank you so much.\"]\n",
    "cleaned_sentence_list = preprocess_text(sentence_list)\n",
    "words_to_index = {word: id for word, id in english_tokenizer_train.word_index.items()}\n",
    "preprocessed_sentence_list = []\n",
    "for sentence in cleaned_sentence_list:\n",
    "    preprocessed_sentence = []\n",
    "    for word in sentence.split():\n",
    "        preprocessed_sentence.append(words_to_index[word.lower()])\n",
    "    preprocessed_sentence_list.append(preprocessed_sentence)\n",
    "tmp_y = pad(preproc_english_sentences_test, max_english_sequence_length)\n",
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "output = (logits_to_text(embeded_model.predict(tmp_x[:5])[0], korean_tokenizer_train))\n",
    "print(output.replace(\"<PAD>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iz59xmu0E9We"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "nlp_assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
